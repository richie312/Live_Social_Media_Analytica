source('get_sentiment.R')
library(StanfordCoreNLP)
library(NLP)
library(dplyr)
setwd('C:/Users/aritra.chatterjee/Desktop/Live_Social_Media_Analytica')
batch1_comments<-read.csv('batch1_comments.csv', stringsAsFactors = FALSE)
colnames(batch1_comments)<-c("ID","comments")
get_POS<-function(text){
## Remove the invalid characters
text1=gsub("https://","",text)
text2= gsub(",","",text1)
text3=gsub("#","",text2)
text4=gsub(">","",text3)
text5=gsub("<","",text4)
text6=gsub("/","",text5)
text7=gsub("[[:punct:]]","",text6)
# Annotation Pipline
s<-text7
p <- StanfordCoreNLP_Pipeline(c("pos", "lemma", "parse"))
doc <- AnnotatedPlainTextDocument(s, p(s))
pos_term<-tagged_sents(doc)
## Get the POS tagging for each and every sentecnce of ,the selected videoo ID
return(term)
}
pos_tags = vector('list',nrow(batch1_comments))
pos_tags
for (i in 1:length(pos_tags)){
pos_tags[i]=get_POS(batch1_comments$comments[i])
}
get_POS<-function(text){
## Remove the invalid characters
text1=gsub("https://","",text)
text2= gsub(",","",text1)
text3=gsub("#","",text2)
text4=gsub(">","",text3)
text5=gsub("<","",text4)
text6=gsub("/","",text5)
text7=gsub("[[:punct:]]","",text6)
# Annotation Pipline
s<-text7
p <- StanfordCoreNLP_Pipeline(c("pos", "lemma", "parse"))
doc <- AnnotatedPlainTextDocument(s, p(s))
pos_term<-tagged_sents(doc)
## Get the POS tagging for each and every sentecnce of ,the selected videoo ID
}
pos_tags = vector('list',nrow(batch1_comments))
for (i in 1:length(pos_tags)){
pos_tags[i]=get_POS(batch1_comments$comments[i])
}
pos_tags
pos_term<-vector('list',length(pos_tags))
term<-vector('list',length(pos_tags))
# get the length of sentence wrt pos terms
for (i in 1:length(pos_tags)){
a<-vector('list',length(pos_tags[[i]]))
b<-vector('list',length(pos_tags[[i]]))
for(j in 1:length(pos_tags[[i]])){
a[j]<-strsplit(as.character(pos_tags[[i]][j]),"/")[1]
b[j]<-strsplit(as.character(pos_tags[[i]][j]),"/")[2]
}
term[[i]]<- a
pos_term[[i]]<-b
}
pos_abb=read.delim2('pos_abb.txt',sep=' ',header = FALSE)
pos_abb=as.data.frame(pos_abb)
colnames(pos_abb)=c("Abb","POS","add_param")
pos_abb$Abb=as.character(pos_abb$Abb)
pos_abb$POS=as.character(pos_abb$POS)
pos_abb$add_param=as.character(pos_abb$add_param)
Noun_Index = which(pos_abb$POS == 'Noun')
Adverb_Index = which(pos_abb$POS == 'Adverb')
pos_term_extend=unlist(pos_term)
term_extend=unlist(term)
term_df=cbind(pos_term_extend,term_extend)%>%as.data.frame()
colnames(term_df)= c("POS","Words")
term_df$Words = as.character(term_df$Words)
term_df$POS = as.character(term_df$POS)
Noun_terms=term_df$Words[term_df$POS==pos_abb$Abb[Noun_Index[1]]|term_df$POS==pos_abb$Abb[Noun_Index[2]]]
Noun_terms
tabulate(Noun_terms)
Noun_words=as.character(Noun_terms)
tabulate(Noun_terms)
Noun_words=as.factor(Noun_terms)
tabulate(Noun_terms)
Noun_terms=term_df$Words[term_df$POS==pos_abb$Abb[Noun_Index[1]]|term_df$POS==pos_abb$Abb[Noun_Index[2]]]
class(Noun_terms)
Noun_words=as.factor(Noun_terms)
tabulate(Noun_words)
table(Noun_words)
Noun_freq=table(Noun_words)
class(Noun_freq)
Noun_freq=table(Noun_words)%>%as.data.frame()
class(Noun_freq)
Noun_freq$Noun_words
head(Noun_freq)
Noun_freq[,Noun_freq$Freq == 3]
Noun_freq[Noun_freq$Freq == 3,]
Noun_freq[Noun_freq$Freq >= 3,]
Noun_freq[Noun_freq$Freq >= 7,]
get_Noun_freq<-function(freq=10){Noun_terms=term_df$Words[term_df$POS==pos_abb$Abb[Noun_Index[1]]|term_df$POS==pos_abb$Abb[Noun_Index[2]]]
Noun_words=as.factor(Noun_terms)
Noun_freq=table(Noun_words)%>%as.data.frame()
Noun_freq=Noun_freq[Noun_freq$Freq >= freq,]
return(Noun_freq)
}
get_Adverb_freq<-function(freq=10){Adverb_terms=term_df$Words[term_df$POS==pos_abb$Abb[Adverb_Index[1]]|term_df$POS==pos_abb$Abb[Adverb_Index[2]]|term_df$POS==pos_abb$Abb[Adverb_Index[3]]]
Aderb_words=as.factor(Adverb_terms)
Adverb_freq=table(Aderb_words)%>%as.data.frame()
Aderb_freq=Noun_freq[adverb_freq$Freq >= freq,]
return(Adverb_freq)
}
get_Adverb_freq(10)
get_Adverb_freq<-function(freq=10){Adverb_terms=term_df$Words[term_df$POS==pos_abb$Abb[Adverb_Index[1]]|term_df$POS==pos_abb$Abb[Adverb_Index[2]]|term_df$POS==pos_abb$Abb[Adverb_Index[3]]]
Aderb_words=as.factor(Adverb_terms)
Adverb_freq=table(Aderb_words)%>%as.data.frame()
Aderb_freq=Noun_freq[Adverb_freq$Freq >= freq,]
return(Adverb_freq)
}
get_Adverb_freq(10)
get_Adverb_freq(freq=10)
Adverb_Index[3]
pos_abb$Abb[Adverb_Index[3]]
pos_abb$Abb[Adverb_Index[2]]
pos_abb$Abb[Adverb_Index[1]]
Aderb_words=as.factor(Adverb_terms)
Adverb_terms=term_df$Words[term_df$POS==pos_abb$Abb[Adverb_Index[1]]|term_df$POS==pos_abb$Abb[Adverb_Index[2]]|term_df$POS==pos_abb$Abb[Adverb_Index[3]]]
Aderb_words=as.factor(Adverb_terms)
Aderb_words
Adverb_freq=table(Aderb_words)%>%as.data.frame()
Adverb_freq=Adverb_freq[Adverb_freq$Freq >= freq,]
Adverb_words=as.factor(Adverb_terms)
Adverb_freq=table(Aderb_words)%>%as.data.frame()
Adverb_freq=Adverb_freq[Adverb_freq$Freq >= freq,]
Adverb_words=as.factor(Adverb_terms)
Adverb_freq=table(Adverb_words)%>%as.data.frame()
Adverb_freq=Adverb_freq[Adverb_freq$Freq >= freq,]
Adverb_terms=term_df$Words[term_df$POS==pos_abb$Abb[Adverb_Index[1]]|term_df$POS==pos_abb$Abb[Adverb_Index[2]]|term_df$POS==pos_abb$Abb[Adverb_Index[3]]]
Adverb_words=as.factor(Adverb_terms)
Adverb_freq=table(Adverb_words)%>%as.data.frame()
Adverb_freq
freq
Adverb_freq=Adverb_freq[Adverb_freq$Freq >= 10,]
Adverb_freq
get_Adverb_freq<-function(freq=5){Adverb_terms=term_df$Words[term_df$POS==pos_abb$Abb[Adverb_Index[1]]|term_df$POS==pos_abb$Abb[Adverb_Index[2]]|term_df$POS==pos_abb$Abb[Adverb_Index[3]]]
Adverb_words=as.factor(Adverb_terms)
Adverb_freq=table(Adverb_words)%>%as.data.frame()
Adverb_freq=Adverb_freq[Adverb_freq$Freq >= freq,]
return(Adverb_freq)
}
get_Adverb_freq(freq=5)
wordcloud2(terms,size=1.5,color="random-light")
library(wordcloud2)
wordcloud2(terms,size=1.5,color="random-light")
terms<-get_Adverb_freq()
terms
wordcloud2(terms$Adverb_words,size=1.5,color="random-light")
terms$Adverb_words
terms<-terms[terms$Freq>=10,]
terms
wordcloud2(terms,size=1.5,color="random-light")
wordcloud2(terms,size=1.5,color="random-light")
terms<-terms[terms$Freq>=1,]
wordcloud2(terms,size=1,color="random-light")
terms<-get_Adverb_freq()
wordcloud2(terms,size=1,color="random-light")
terms<-get_Noun_freq()
wordcloud2(terms,size=1,color="random-light")
shiny::runApp()
save(pos_tags, file = "pos_tags.RData")
pos_tags = load('pos_tags.Rdata')
pos_tags
pos_tags = load('pos_tags.RData')
pos_tags
pos_tags = readRDS('pos_tags.RData')
pos_tags = readRDS('pos_tags.RDS')
saveRDS(pos_tags,file='pos_tags.RDS')
pos_tags = readRDS('pos_tags.RDS')
pos_tags
pos_tags = vector('list',nrow(batch1_comments))
for (i in 1:length(pos_tags)){
pos_tags[i]=get_POS(batch1_comments$comments[i])
}
source('get_sentiment.R')
library(StanfordCoreNLP)
library(NLP)
library(dplyr)
#setwd('C:/Users/aritra.chatterjee/Desktop/Live_Social_Media_Analytica')
batch1_comments<-read.csv('batch1_comments.csv', stringsAsFactors = FALSE)
colnames(batch1_comments)<-c("ID","comments")
get_POS<-function(text){
## Remove the invalid characters
text1=gsub("https://","",text)
text2= gsub(",","",text1)
text3=gsub("#","",text2)
text4=gsub(">","",text3)
text5=gsub("<","",text4)
text6=gsub("/","",text5)
text7=gsub("[[:punct:]]","",text6)
# Annotation Pipline
s<-text7
p <- StanfordCoreNLP_Pipeline(c("pos", "lemma", "parse"))
doc <- AnnotatedPlainTextDocument(s, p(s))
pos_term<-tagged_sents(doc)
## Get the POS tagging for each and every sentecnce of ,the selected videoo ID
}
# Get the taggings and store it
# Instantiate the empty dataframe in order to store the terms
pos_tags = vector('list',nrow(batch1_comments))
for (i in 1:length(pos_tags)){
pos_tags[i]=get_POS(batch1_comments$comments[i])
}
save(pos_tags, file = "pos_tags.RData")
saveRDS(pos_tags,file='pos_tags.RDS')
pos_tags = readRDS('pos_tags.RDS')
pos_tags
shiny::runApp()
runApp()
