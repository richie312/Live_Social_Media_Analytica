}
return(sentences)
}
word='caramel:1'
get_sentences(strsplit(word,":")[[1]][1])
get_sentences(strsplit(word,":")[[1]][1])[!unlist(lapply(get_sentences(strsplit(word,":")[[1]][1]), is.null))]
get_sentences= function(word){
# Instantiate empt;y list to store sentences from the loop
sentences = list(rep(NA, 100))
for (i in 1:length(user_loc_df$text)){
if (grepl(word,strsplit(user_loc_df$text[i]," ")) == TRUE){
sentences[i]=user_loc_df$text[i]
}
}
return(sentences)
}
word='caramel:1'
get_sentences(strsplit(word,":")[[1]][1])
get_sentences(strsplit(word,":")[[1]][1])[!unlist(lapply(get_sentences(strsplit(word,":")[[1]][1]), is.null))]
get_sentences= function(word){
# Instantiate empt;y list to store sentences from the loop
for (i in 1:length(user_loc_df$text)){
if (grepl(word,strsplit(user_loc_df$text[i]," ")) == TRUE){
sentences[i]=user_loc_df$text[i]
}
}
return(sentences)
}
word='caramel:1'
get_sentences(strsplit(word,":")[[1]][1])
get_sentences= function(word){
# Instantiate empt;y list to store sentences from the loop
for (i in 1:length(user_loc_df$text)){
if (grepl(word,strsplit(user_loc_df$text[i]," ")) == TRUE){
sentences[i]=user_loc_df$text[i]
}
}
return(sentences)
}
word='caramel:1'
get_sentences(strsplit(word,":")[[1]][1])
get_sentences= function(word){
# Instantiate empt;y list to store sentences from the loop
for (i in 1:length(user_loc_df$text)){
if (grepl(word,strsplit(user_loc_df$text[i]," ")) == TRUE){
sentences[i]=user_loc_df$text[i]
}
return(sentences)
}
}
word='caramel:1'
get_sentences(strsplit(word,":")[[1]][1])
sentences
sentences=list()
type(sentences)
}
type(sentences)
class(sentences)
length(sentences)
sentences[1]
sentences[0]
sentences[1:100]
get_sentences= function(word){
# Instantiate empty list to store sentences from the loop
sentences=list()
for (i in 1:length(user_loc_df$text)){
if (grepl(word,strsplit(user_loc_df$text[i]," ")) == TRUE){
sentences[i]=user_loc_df$text[i]
}
return(sentences)
}
}
get_sentences= function(word){
# Instantiate empty list to store sentences from the loop
sentences=list()
for (i in 1:length(user_loc_df$text)){
if (grepl(word,strsplit(user_loc_df$text[i]," ")) == TRUE){
sentences[i]=user_loc_df$text[i]
}
return(sentences)
}
}
word='caramel:1'
get_sentences(strsplit(word,":")[[1]][1])
get_sentences= function(word){
# Instantiate empty list to store sentences from the loop
sentences=list()
for (i in 1:length(user_loc_df$text)){
if (grepl(word,strsplit(user_loc_df$text[i]," ")) == TRUE){
sentences[i]=user_loc_df$text[i]
}
}
return(sentences)
}
word='caramel:1'
get_sentences(strsplit(word,":")[[1]][1])
get_sentences(strsplit(word,":")[[1]][1])[!unlist(lapply(get_sentences(strsplit(word,":")[[1]][1]), is.null))]
get_sentences= function(word){
# Instantiate empty list to store sentences from the loop
sentences=list()
for (i in 1:length(user_loc_df$text)){
if (grepl(word,strsplit(user_loc_df$text[i]," ")) == TRUE){
sentences[i]=user_loc_df$text[i]
}
}
return(sentences)
}
word='caramel:1'
get_sentences(strsplit(word,":")[[1]][1])
get_sentences(strsplit(word,":")[[1]][1])[!unlist(lapply(get_sentences(strsplit(word,":")[[1]][1]), is.null))]
runApp()
runApp()
get_sentences('cpress')
get_sentences('starbucks')
get_sentences('cpress')
get_sentences('the')
?wordcloud2()
words<-get_words(data$text)
data = read.csv('user_loc_df.csv',stringsAsFactors = FALSE)
words<-get_words(data$text)
names(words)
words[words$freq > 10,]
words[words$freq > 1,]
get_sentences('ueueueueueue')
get_sentences('starbucks')
user_loc_df$text
stopwords(kind = "en")
stopwords('SMART')
get_words<-function(text){
Text<- text
Text<-iconv(Text, "latin1", "UTF-8")
##Clean the tweet for sentiment analysis
#  remove html links, which are not required for sentiment analysis
tweet1=gsub("https://","",Text)
tweet2=gsub("#","",tweet1)
tweet3=gsub("t.co/","",tweet2)
tweet4=gsub("@","",tweet3)
tweet5=gsub("RT|via","",tweet4)
tweet6=gsub("[[:digit:]]","",tweet5)
tweet7=gsub("'\'"," ", tweet6)
tweet7<-tweet7%>%data.frame()
colnames(tweet7)<-c("Tweets")
## Load the tm package to clean the corpus
## Corpus
Corpus=Corpus(VectorSource(tweet7$Tweets))
## Convert to plain text document
Corpus=tm_map(Corpus,PlainTextDocument)
## lower case
Corpus=tm_map(Corpus,content_transformer(tolower))
## Strip Whitespace
Corpus<-tm_map(Corpus, stripWhitespace)
# Remove Punctuation
Corpus=tm_map(Corpus,removePunctuation)
Corpus = tm_map(documents, removeWords, stopwords('SMART'))
Corpus = tm_map(documents, removeWords, stopwords('en'))
## Document term matrix
myDTM = TermDocumentMatrix(Corpus)
m = as.matrix(myDTM)
v = sort(rowSums(m), decreasing = TRUE)
d<-data.frame(word=names(v),freq=v)
return(d)
}
runApp()
data = read.csv('user_loc_df.csv',stringsAsFactors = FALSE)
words<-get_words(data$text)
words
wordcloud2(words,size=0.6,color="random-light")
Text<-user_loc_df$text
Text
tweet1=gsub("https://","",Text)
tweet2=gsub("#","",tweet1)
tweet3=gsub("t.co/","",tweet2)
tweet4=gsub("@","",tweet3)
tweet5=gsub("RT|via","",tweet4)
tweet6=gsub("[[:digit:]]","",tweet5)
tweet7=gsub("'\'"," ", tweet6)
tweet7<-tweet7%>%data.frame()
colnames(tweet7)<-c("Tweets")
Corpus=Corpus(VectorSource(tweet7$Tweets))
Corpus=tm_map(Corpus,PlainTextDocument)
Corpus=tm_map(Corpus,content_transformer(tolower))
## Strip Whitespace
Corpus<-tm_map(Corpus, stripWhitespace)
# Remove Punctuation
Corpus=tm_map(Corpus,removePunctuation)
Corpus = tm_map(documents, removeWords, stopwords('SMART'))
Corpus = tm_map(documents, removeWords, stopwords('en'))
## Document term matrix
myDTM = TermDocumentMatrix(Corpus)
m = as.matrix(myDTM)
v = sort(rowSums(m), decreasing = TRUE)
d<-data.frame(word=names(v),freq=v)
d
stopwords('en')
stopwords('english')
read.delim("stopwords.txt")
stopwords = read.delim("stopwords.txt")
stopwords
stopwords = read.delim("stopwords.txt",sep=',')
stopwords
stopwords
Corpus = tm_map(documents, removeWords, stopwords('SMART'))
Corpus = tm_map(documents, removeWords, stopwords)
Corpus=tm_map(Corpus,removePunctuation)
Corpus = tm_map(documents, removeWords, stopwords('SMART'))
Corpus=tm_map(Corpus,removePunctuation)
Corpus = tm_map(Corpus, removeWords, stopwords('SMART'))
Corpus = tm_map(Corpus, removeWords, stopwords)
myDTM = TermDocumentMatrix(Corpus)
m = as.matrix(myDTM)
v = sort(rowSums(m), decreasing = TRUE)
d<-data.frame(word=names(v),freq=v)
d
runApp()
runApp()
Text<-iconv(Text, "latin1", "UTF-8")
runApp()
runApp()
runApp()
iconv('cpress', "UTF-8", "latin1")
stopwords = read.delim("stopwords.txt",sep=',')
Text<- text
#Text<-iconv(Text, "latin1", "UTF-8")
##Clean the tweet for sentiment analysis
#  remove html links, which are not required for sentiment analysis
tweet1=gsub("https://","",Text)
tweet2=gsub("#","",tweet1)
tweet3=gsub("t.co/","",tweet2)
tweet4=gsub("@","",tweet3)
tweet5=gsub("RT|via","",tweet4)
tweet6=gsub("[[:digit:]]","",tweet5)
tweet7=gsub("'\'"," ", tweet6)
tweet7<-tweet7%>%data.frame()
colnames(tweet7)<-c("Tweets")
## Load the tm package to clean the corpus
## Corpus
Corpus=Corpus(VectorSource(tweet7$Tweets))
## Convert to plain text document
Corpus=tm_map(Corpus,PlainTextDocument)
## lower case
Corpus=tm_map(Corpus,content_transformer(tolower))
## Strip Whitespace
Corpus<-tm_map(Corpus, stripWhitespace)
# Remove Punctuation
Corpus=tm_map(Corpus,removePunctuation)
Corpus = tm_map(Corpus, removeWords, stopwords('SMART'))
Corpus = tm_map(Corpus, removeWords, stopwords)
## Document term matrix
myDTM = TermDocumentMatrix(Corpus)
m = as.matrix(myDTM)
v = sort(rowSums(m), decreasing = TRUE)
d<-data.frame(word=names(v),freq=v)
d
text
Text<- user_loc_df$text
Text
stopwords = read.delim("stopwords.txt",sep=',')
Text<- user_loc_df$text
#Text<-iconv(Text, "latin1", "UTF-8")
##Clean the tweet for sentiment analysis
#  remove html links, which are not required for sentiment analysis
tweet1=gsub("https://","",Text)
tweet2=gsub("#","",tweet1)
tweet3=gsub("t.co/","",tweet2)
tweet4=gsub("@","",tweet3)
tweet5=gsub("RT|via","",tweet4)
tweet6=gsub("[[:digit:]]","",tweet5)
tweet7=gsub("'\'"," ", tweet6)
tweet7<-tweet7%>%data.frame()
colnames(tweet7)<-c("Tweets")
## Load the tm package to clean the corpus
## Corpus
Corpus=Corpus(VectorSource(tweet7$Tweets))
## Convert to plain text document
Corpus=tm_map(Corpus,PlainTextDocument)
## lower case
Corpus=tm_map(Corpus,content_transformer(tolower))
## Strip Whitespace
Corpus<-tm_map(Corpus, stripWhitespace)
# Remove Punctuation
Corpus=tm_map(Corpus,removePunctuation)
Corpus = tm_map(Corpus, removeWords, stopwords('SMART'))
Corpus = tm_map(Corpus, removeWords, stopwords)
## Document term matrix
myDTM = TermDocumentMatrix(Corpus)
m = as.matrix(myDTM)
v = sort(rowSums(m), decreasing = TRUE)
d<-data.frame(word=names(v),freq=v)
d
wordcloud2(d,size=0.6,color="random-light",langs='en')
?wordcloud2
wordcloud2(d,size=0.6,color="random-light")
Corpus=tm_map(Corpus,removePunctuation)
Corpus=tm_map(Corpus,toSpace,"(f|ht)tp(s?)://(.*)[.][a-z]+")
Corpus = tm_map(Corpus, removeWords, stopwords('SMART'))
Corpus = tm_map(Corpus, removeWords, stopwords)
myDTM = TermDocumentMatrix(Corpus)
m = as.matrix(myDTM)
v = sort(rowSums(m), decreasing = TRUE)
d<-data.frame(word=names(v),freq=v)
d
wordcloud2(d,size=0.6,color="random-light")
stopwords = read.delim("stopwords.txt",sep=',')
Text<- user_loc_df$text
#Text<-iconv(Text, "latin1", "UTF-8")
##Clean the tweet for sentiment analysis
#  remove html links, which are not required for sentiment analysis
tweet1=gsub("https://","",Text)
tweet2=gsub("#","",tweet1)
tweet3=gsub("t.co/","",tweet2)
tweet4=gsub("@","",tweet3)
tweet5=gsub("\\u003c/","",tweet4)
tweet6=gsub("[[:digit:]]","",tweet5)
tweet7=gsub("'\'"," ", tweet6)
tweet7<-tweet7%>%data.frame()
colnames(tweet7)<-c("Tweets")
## Load the tm package to clean the corpus
## Corpus
Corpus=Corpus(VectorSource(tweet7$Tweets))
## Convert to plain text document
Corpus=tm_map(Corpus,PlainTextDocument)
## lower case
Corpus=tm_map(Corpus,content_transformer(tolower))
## Strip Whitespace
Corpus<-tm_map(Corpus, stripWhitespace)
# Remove Punctuation
Corpus=tm_map(Corpus,removePunctuation)
Corpus=tm_map(Corpus,toSpace,"(f|ht)tp(s?)://(.*)[.][a-z]+")
Corpus = tm_map(Corpus, removeWords, stopwords('SMART'))
Corpus = tm_map(Corpus, removeWords, stopwords)
## Document term matrix
myDTM = TermDocumentMatrix(Corpus)
m = as.matrix(myDTM)
v = sort(rowSums(m), decreasing = TRUE)
d<-data.frame(word=names(v),freq=v)
d
wordcloud2(d,size=0.6,color="random-light")
stopwords = read.delim("stopwords.txt",sep=',')
Text<- user_loc_df$text
Text<-iconv(Text, "latin1", "UTF-8",sub='')
##Clean the tweet for sentiment analysis
#  remove html links, which are not required for sentiment analysis
tweet1=gsub("https://","",Text)
tweet2=gsub("#","",tweet1)
tweet3=gsub("t.co/","",tweet2)
tweet4=gsub("@","",tweet3)
tweet5=gsub("\\u003c/","",tweet4)
tweet6=gsub("[[:digit:]]","",tweet5)
tweet7=gsub("'\'"," ", tweet6)
tweet7<-tweet7%>%data.frame()
colnames(tweet7)<-c("Tweets")
Corpus=Corpus(VectorSource(tweet7$Tweets))
## Convert to plain text document
Corpus=tm_map(Corpus,PlainTextDocument)
## lower case
Corpus=tm_map(Corpus,content_transformer(tolower))
## Strip Whitespace
Corpus<-tm_map(Corpus, stripWhitespace)
# Remove Punctuation
Corpus=tm_map(Corpus,removePunctuation)
Corpus=tm_map(Corpus,toSpace,"(f|ht)tp(s?)://(.*)[.][a-z]+")
Corpus = tm_map(Corpus, removeWords, stopwords('SMART'))
Corpus = tm_map(Corpus, removeWords, stopwords)
## Document term matrix
myDTM = TermDocumentMatrix(Corpus)
m = as.matrix(myDTM)
v = sort(rowSums(m), decreasing = TRUE)
d<-data.frame(word=names(v),freq=v)
d
wordcloud2(d,size=0.6,color="random-light")
runApp()
runApp()
stopwords = read.delim("stopwords.txt",sep=',')
Text<-iconv(Text, "UTF-8", "UTF-8",sub='')
tweet1=gsub("https://","",Text)
tweet2=gsub("#","",tweet1)
tweet3=gsub("t.co/","",tweet2)
tweet4=gsub("@","",tweet3)
tweet5=gsub("RT|via","",tweet4)
tweet6=gsub("[[:digit:]]","",tweet5)
tweet7=gsub("'\'"," ", tweet6)
tweet7<-tweet7%>%data.frame()
colnames(tweet7)<-c("Tweets")
## Load the tm package to clean the corpus
## Corpus
Corpus=Corpus(VectorSource(tweet7$Tweets))
## Convert to plain text document
Corpus=tm_map(Corpus,PlainTextDocument)
## lower case
Corpus=tm_map(Corpus,content_transformer(tolower))
## Strip Whitespace
Corpus<-tm_map(Corpus, stripWhitespace)
# Remove Punctuation
Corpus=tm_map(Corpus,removePunctuation)
Corpus = tm_map(Corpus, removeWords, stopwords('SMART'))
Corpus = tm_map(Corpus, removeWords, stopwords)
## Document term matrix
myDTM = TermDocumentMatrix(Corpus)
m = as.matrix(myDTM)
v = sort(rowSums(m), decreasing = TRUE)
d<-data.frame(word=names(v),freq=v)
d
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
get_sentences= function(word){
# Instantiate empty list to store sentences from the loop
sentences=list()
for (i in 1:length(user_loc_df$text)){
if (grepl(word,strsplit(user_loc_df$text[i]," ")) == TRUE){
sentences[i]=user_loc_df$text[i]
}
else{sentences[i] = "This word cannot be traced back to tweets. It might be result of conversion from non-UTF character to UTF-8"}
}
return(sentences)
}
get_sentences('popedik')
return(unique(sentences))
get_sentences= function(word){
# Instantiate empty list to store sentences from the loop
sentences=list()
for (i in 1:length(user_loc_df$text)){
if (grepl(word,strsplit(user_loc_df$text[i]," ")) == TRUE){
sentences[i]=user_loc_df$text[i]
}
else{sentences[i] = "This word cannot be traced back to tweets. It might be result of conversion from non-UTF character to UTF-8"}
}
return(unique(sentences))
}
get_sentences('popedik')
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
get_sentences('disney')
a=get_sentences('disney')
type(a)
class(a)
get_sentences= function(word){
# Instantiate empty list to store sentences from the loop
sentences=list()
for (i in 1:length(user_loc_df$text)){
if (grepl(word,strsplit(user_loc_df$text[i]," ")) == TRUE){
sentences[i]=user_loc_df$text[i]
}
}
else{"Not valid UTF character"}
return(unique(sentences))
}
a=get_sentences('disney')
a
get_sentences('coffee')
get_sentences= function(word){
# Instantiate empty list to store sentences from the loop
sentences=list()
for (i in 1:length(user_loc_df$text)){
if (grepl(word,strsplit(user_loc_df$text[i]," ")) == TRUE){
sentences[i]=user_loc_df$text[i]
}
else{"Not valid UTF character"}
}
return(unique(sentences))
}
get_sentences('coffee')
get_sentences= function(word){
# Instantiate empty list to store sentences from the loop
sentences=list()
for (i in 1:length(user_loc_df$text)){
if (grepl(word,strsplit(user_loc_df$text[i]," ")) == TRUE){
sentences[i]=user_loc_df$text[i]
}
else{print("Not valid UTF character")}
}
return(unique(sentences))
}
get_sentences('coffee')
get_sentences= function(word){
# Instantiate empty list to store sentences from the loop
sentences=list()
for (i in 1:length(user_loc_df$text)){
if (grepl(word,strsplit(user_loc_df$text[i]," ")) == TRUE){
sentences[i]=user_loc_df$text[i]
}
else{sentences="Not valid UTF character"}
}
return(unique(sentences))
}
get_sentences('coffee')
get_sentences= function(word){
# Instantiate empty list to store sentences from the loop
sentences=list()
for (i in 1:length(user_loc_df$text)){
if (grepl(word,strsplit(user_loc_df$text[i]," ")) == TRUE){
sentences[i]=user_loc_df$text[i]
}
else{}
}
return(unique(sentences))
}
get_sentences('coffee')
sent = get_sentences(strsplit(input$wc2_clicked_word,":")[[1]][1])
runApp()
