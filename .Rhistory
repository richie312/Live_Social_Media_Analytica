th.start <- pi*(1-a/100)
th.end   <- pi*(1-b/100)
th       <- seq(th.start,th.end,length=100)
x        <- c(r1*cos(th),rev(r2*cos(th)))
y        <- c(r1*sin(th),rev(r2*sin(th)))
return(data.frame(x,y))
}
ggplot()+
geom_polygon(data=get.poly(breaks[1],breaks[2]),aes(x,y),fill="#008B8B")+
geom_polygon(data=get.poly(breaks[2],breaks[3]),aes(x,y),fill="#00CED1")+
geom_polygon(data=get.poly(breaks[3],breaks[4]),aes(x,y),fill="#AFEEEE")+
geom_polygon(data=get.poly(pos-1,pos+1,0.2),aes(x,y))+
geom_text(data=as.data.frame(breaks), size=5, fontface="bold", vjust=0,
aes(x=1.1*cos(pi*(1-breaks/100)),y=1.1*sin(pi*(1-breaks/100)),label=paste0(breaks,"%")))+
#annotate("text",x=0,y=0,label=pos,vjust=0,size=8,fontface="bold")+
coord_fixed()+
theme_bw()+
theme(axis.text=element_blank(),
axis.title=element_blank(),
axis.ticks=element_blank(),
panel.grid=element_blank(),
panel.border=element_blank())
}
gg.gauge(0.88*100,breaks=c(0,30,70,100))
runApp()
runApp()
library(ggplot2)
gg.gauge <- function(pos,breaks=c(0,30,70,100)) {
get.poly <- function(a,b,r1=0.5,r2=1.0) {
th.start <- pi*(1-a/100)
th.end   <- pi*(1-b/100)
th       <- seq(th.start,th.end,length=100)
x        <- c(r1*cos(th),rev(r2*cos(th)))
y        <- c(r1*sin(th),rev(r2*sin(th)))
return(data.frame(x,y))
}
ggplot()+
geom_polygon(data=get.poly(breaks[1],breaks[2]),aes(x,y),fill="#008B8B")+
geom_polygon(data=get.poly(breaks[2],breaks[3]),aes(x,y),fill="#00CED1")+
geom_polygon(data=get.poly(breaks[3],breaks[4]),aes(x,y),fill="#AFEEEE")+
geom_polygon(data=get.poly(pos-1,pos+1,0.2),aes(x,y))+
geom_text(data=as.data.frame(breaks), size=5, fontface="bold", vjust=0,
aes(x=1.1*cos(pi*(1-breaks/100)),y=1.1*sin(pi*(1-breaks/100)),label=paste0(breaks,"%")))+
#annotate("text",x=0,y=0,label=pos,vjust=0,size=8,fontface="bold")+
coord_fixed()+
theme_bw()+
theme(axis.text=element_blank(),
axis.title=element_blank(),
axis.ticks=element_blank(),
panel.grid=element_blank(),
panel.border=element_blank())
}
gg.gauge <- function(pos,breaks=c(0,30,70,100)) {
get.poly <- function(a,b,r1=0.5,r2=1.0) {
th.start <- pi*(1-a/100)
th.end   <- pi*(1-b/100)
th       <- seq(th.start,th.end,length=100)
x        <- c(r1*cos(th),rev(r2*cos(th)))
y        <- c(r1*sin(th),rev(r2*sin(th)))
return(data.frame(x,y))
}
ggplot()+
geom_polygon(data=get.poly(breaks[1],breaks[2]),aes(x,y),fill="#008B8B")+
geom_polygon(data=get.poly(breaks[2],breaks[3]),aes(x,y),fill="#00CED1")+
geom_polygon(data=get.poly(breaks[3],breaks[4]),aes(x,y),fill="#AFEEEE")+
geom_polygon(data=get.poly(pos-1,pos+1,0.2),aes(x,y))+
geom_text(data=as.data.frame(breaks), size=5, fontface="bold", vjust=0,
aes(x=1.1*cos(pi*(1-breaks/100)),y=1.1*sin(pi*(1-breaks/100)),label=paste0(breaks,"%")))+
#annotate("text",x=0,y=0,label=pos,vjust=0,size=8,fontface="bold")+
coord_fixed()+
theme_bw()+
theme(axis.text=element_blank(),
axis.title=element_blank(),
axis.ticks=element_blank(),
panel.grid=element_blank(),
panel.border=element_blank())
}
runApp()
video_stats<-read.csv('video_stat.csv',stringsAsFactors = FALSE)
data<-video_stats[video_stats$id == 'XUBeH7VQaFY',]
data$view_Count=as.numeric(data$view_Count/10)
data$like_Count=as.numeric(data$like_Count)
if (is.na(data$like_Count)){
data$like_Count = 0
}
value = data$like_Count/data$view_Count
gg.gauge(value*100,breaks=c(0,30,70,100))
value = data$like_Count/data$view_Count
data$view_Count=as.numeric(data$view_Count/10)
data$like_Count=as.numeric(data$like_Count)
data$view_Count=as.numeric(data$view_Count/10)
data$like_Count=as.numeric(data$like_Count)
data$view_Count=as.numeric(data$view_Count)
data$like_Count=as.numeric(data$like_Count)
value = data$like_Count/(data$view_Count/10)
gg.gauge(value*100,breaks=c(0,30,70,100))
runApp()
runApp()
runApp()
source('get_sentiment.R')
library(StanfordCoreNLP)
library(NLP)
library(dplyr)
#setwd('C:/Users/aritra.chatterjee/Desktop/Live_Social_Media_Analytica')
batch1_comments<-read.csv('batch1_comments.csv', stringsAsFactors = FALSE)
colnames(batch1_comments)<-c("ID","comments")
get_bigram<-function(text){
sentences<-text
# Clean
clean<-function(sentences){
text1=gsub("https://","",sentences)
text2= gsub(",","",text1)
text3=gsub("#","",text2)
text4=gsub(">","",text3)
text5=gsub("<","",text4)
text6=gsub("/","",text5)
text7=gsub("[[:punct:]]","",text6)
}
## Corpus
corpus = VCorpus(VectorSource(clean(sentences)))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeWords, stopwords("english"))
## Bigram Tokenizer function
Bigramtokenizer<-function(x){
unlist(lapply(ngrams(words(x),2),paste,collapse =" "),use.names = FALSE)
}
## Creating doc matrix
bigramdtm <- TermDocumentMatrix(corpus,control = list(tokenize = Bigramtokenizer))
# Find frequency
bigramf <- findFreqTerms(bigramdtm,lowfreq = 15)
# Convert it into the data form
Bigram_Terms<-rowSums(as.matrix(bigramdtm[bigramf,]))
Bigram_Terms<-data.frame(word=names(Bigram_Terms),frequency=Bigram_Terms)
return(Bigram_Terms)
}
get_bigram(batch1_comments$comments)
get_bigram(batch1_comments$comments)
get_trigram<-function(text){
sentences<-text
# Clean
clean<-function(sentences){
text1=gsub("https://","",sentences)
text2= gsub(",","",text1)
text3=gsub("#","",text2)
text4=gsub(">","",text3)
text5=gsub("<","",text4)
text6=gsub("/","",text5)
text7=gsub("[[:punct:]]","",text6)
}
## Corpus
corpus = VCorpus(VectorSource(clean(sentences)))
## Bigram Tokenizer function
Trigramtokenizer<-function(x){
unlist(lapply(ngrams(words(x),3),paste,collapse =" "),use.names = FALSE)
}
## Creating doc matrix
Trigramdtm <- TermDocumentMatrix(corpus,control = list(tokenize = Trigramtokenizer))
# Find frequency
Trigramf <- findFreqTerms(Trigramdtm,lowfreq = 5)
# Convert it into the data form
Trigram_Terms<-rowSums(as.matrix(Trigramdtm[Trigramf,]))
Trigram_Terms<-data.frame(word=names(Trigram_Terms),frequency=Trigram_Terms)
return(Trigram_Terms)
}
get_trigram(batch1_comments$comments)
get_trigram<-function(text){
sentences<-text
# Clean
clean<-function(sentences){
text1=gsub("https://","",sentences)
text2= gsub(",","",text1)
text3=gsub("#","",text2)
text4=gsub(">","",text3)
text5=gsub("<","",text4)
text6=gsub("/","",text5)
text7=gsub("[[:punct:]]","",text6)
}
## Corpus
corpus = VCorpus(VectorSource(clean(sentences)))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeWords, stopwords("english"))
## Bigram Tokenizer function
Trigramtokenizer<-function(x){
unlist(lapply(ngrams(words(x),3),paste,collapse =" "),use.names = FALSE)
}
## Creating doc matrix
Trigramdtm <- TermDocumentMatrix(corpus,control = list(tokenize = Trigramtokenizer))
# Find frequency
Trigramf <- findFreqTerms(Trigramdtm,lowfreq = 5)
# Convert it into the data form
Trigram_Terms<-rowSums(as.matrix(Trigramdtm[Trigramf,]))
Trigram_Terms<-data.frame(word=names(Trigram_Terms),frequency=Trigram_Terms)
return(Trigram_Terms)
}
get_trigram(batch1_comments$comments)
runApp()
runApp()
user_loc_df<-read.csv('user_loc_df.csv',stringsAsFactors = FALSE)
Text<-user_loc_df$text
stopwords = read.delim("stopwords.txt",sep=',')
Text<-iconv(Text, "latin1", "ASCII",sub='')
Text
tweet1=gsub("https://","",Text)
tweet2=gsub("#","",tweet1)
tweet3=gsub("t.co/","",tweet2)
tweet4=gsub("@","",tweet3)
tweet5=gsub("RT|via","",tweet4)
tweet6=gsub("[[:digit:]]","",tweet5)
tweet7=gsub("'\'"," ", tweet6)
text8=gsub("[[:punct:]]","",text7)
tweet1=gsub("https://","",Text)
tweet2=gsub("#","",tweet1)
tweet3=gsub("t.co/","",tweet2)
tweet4=gsub("@","",tweet3)
tweet5=gsub("RT|via","",tweet4)
tweet6=gsub("[[:digit:]]","",tweet5)
tweet7=gsub("/"," ", tweet6)
text8=gsub("[[:punct:]]","",text7)
tweet6=gsub("[[:digit:]]","",tweet5)
tweet7=gsub("/","", tweet6)
tweet1=gsub("https://","",Text)
tweet2=gsub("#","",tweet1)
tweet3=gsub("t.co/","",tweet2)
tweet4=gsub("@","",tweet3)
tweet5=gsub("RT|via","",tweet4)
tweet6=gsub("[[:digit:]]","",tweet5)
tweet7=gsub("'\'"," ", tweet6)
tweet8=gsub("[[:punct:]]","",tweet7)
tweet8<-tweet8%>%data.frame()
colnames(tweet8)<-c("Tweets")
runApp()
Corpus=Corpus(VectorSource(tweet8$Tweets))
Corpus=tm_map(Corpus,PlainTextDocument)
Corpus=tm_map(Corpus,content_transformer(tolower))
Corpus<-tm_map(Corpus, stripWhitespace)
Corpus=tm_map(Corpus,removePunctuation)
Corpus = tm_map(Corpus, removeWords, stopwords('SMART'))
Corpus = tm_map(Corpus, removeWords, stopwords)
myDTM = TermDocumentMatrix(Corpus)
m = as.matrix(myDTM)
v = sort(rowSums(m), decreasing = TRUE)
d<-data.frame(word=names(v),freq=v)
d
data = read.csv('user_loc_df.csv',stringsAsFactors = FALSE)
words<-get_words(data$text)
data$text
words<-get_words(data$text)
words<-words[words$freq>1,]
data = read.csv('user_loc_df.csv',stringsAsFactors = FALSE)
names(data)
Text<-data$text
Text<-iconv(Text, "latin1", "ASCII",sub='')
tweet1=gsub("https://","",Text)
tweet2=gsub("#","",tweet1)
tweet3=gsub("t.co/","",tweet2)
tweet4=gsub("@","",tweet3)
tweet5=gsub("RT|via","",tweet4)
tweet6=gsub("[[:digit:]]","",tweet5)
tweet7=gsub("'\'"," ", tweet6)
tweet8=gsub("[[:punct:]]","",tweet7)
tweet8<-tweet8%>%data.frame()
colnames(tweet8)<-c("Tweets")
## Load the tm package to clean the corpus
## Corpus
Corpus=Corpus(VectorSource(tweet8$Tweets))
## Convert to plain text document
Corpus=tm_map(Corpus,PlainTextDocument)
## lower case
Corpus=tm_map(Corpus,content_transformer(tolower))
## Strip Whitespace
Corpus<-tm_map(Corpus, stripWhitespace)
# Remove Punctuation
Corpus=tm_map(Corpus,removePunctuation)
Corpus = tm_map(Corpus, removeWords, stopwords('SMART'))
Corpus = tm_map(Corpus, removeWords, stopwords)
## Document term matrix
myDTM = TermDocumentMatrix(Corpus)
m = as.matrix(myDTM)
v = sort(rowSums(m), decreasing = TRUE)
d<-data.frame(word=names(v),freq=v)
d
Text
get_words<-function(text){
# user_loc_df<-read.csv('user_loc_df.csv',stringsAsFactors = FALSE)
# Text<-user_loc_df$text
# stopwords = read.delim("stopwords.txt",sep=',')
Text<-iconv(text, "latin1", "ASCII",sub='')
##Clean the tweet for sentiment analysis
#  remove html links, which are not required for sentiment analysis
tweet1=gsub("https://","",Text)
tweet2=gsub("#","",tweet1)
tweet3=gsub("t.co/","",tweet2)
tweet4=gsub("@","",tweet3)
tweet5=gsub("RT|via","",tweet4)
tweet6=gsub("[[:digit:]]","",tweet5)
tweet7=gsub("'\'"," ", tweet6)
tweet8=gsub("[[:punct:]]","",tweet7)
tweet8<-tweet8%>%data.frame()
colnames(tweet8)<-c("Tweets")
## Load the tm package to clean the corpus
## Corpus
Corpus=Corpus(VectorSource(tweet8$Tweets))
## Convert to plain text document
Corpus=tm_map(Corpus,PlainTextDocument)
## lower case
Corpus=tm_map(Corpus,content_transformer(tolower))
## Strip Whitespace
Corpus<-tm_map(Corpus, stripWhitespace)
# Remove Punctuation
Corpus=tm_map(Corpus,removePunctuation)
Corpus = tm_map(Corpus, removeWords, stopwords('SMART'))
Corpus = tm_map(Corpus, removeWords, stopwords)
## Document term matrix
myDTM = TermDocumentMatrix(Corpus)
m = as.matrix(myDTM)
v = sort(rowSums(m), decreasing = TRUE)
d<-data.frame(word=names(v),freq=v)
return(d)
}
get_words(data$text)
words<-get_words(data$text)
words<-words[words$freq>1,]
wordcloud2(words,size=2,color="random-light")
?wordcloud2()
words<-get_words(data$text)
words<-words[words$freq>1,]
wordcloud2(words,size=2,color="random-light",shape='triangle')
wordcloud2(words,size=2,color="random-light",shape='heart')
wordcloud2(words,size=2,color="random-light",shape='square')
wordcloud2(words,size=2,color="random-light",shape='diamond')
wordcloud2(words,size=2,color="random-light",figPath = 'twitter.JPG')
wordcloud2(words,size=2,color="random-light",figPath = 'twitter.jpg')
wordcloud2(words,size=2,color="random-light",figPath = 'twitter.jpg')
runApp()
runApp()
data = read.csv('user_loc_df.csv',stringsAsFactors = FALSE)
data = data%>%select('name', 'location',
'created_at','text','quoted_followers_count','quoted_retweet_count')
data$text=iconv(data$text, "latin1", "ASCII", sub="")
# Instantiate empty list for storing the text polarity score
polarity = vector('List',nrow(data))
for (i in 1:nrow(data)){
polarity[i] = get_sentimentscore(data$text[i])}
data$polarity=polarity[i]
data$text=iconv(data$text, "latin1", "ASCII", sub="")
polarity = vector('List',nrow(data))
polarity = vector('list',nrow(data))
for (i in 1:nrow(data)){
polarity[i] = get_sentimentscore(data$text[i])}
data$polarity=polarity[i]
data$polarity
get_sentimentscore(data$text[1])
a=get_sentimentscore(data$text[1])
names(a)
a$Degree_of_Emotion[a$Number_of_Tweets>0]
get_sentimentscore(data$text[1])$Degree_of_Emotion[a$Number_of_Tweets>0]}
get_sentimentscore(data$text[1])$Degree_of_Emotion[a$Number_of_Tweets>0]
get_sentimentscore(data$text[10])$Degree_of_Emotion[a$Number_of_Tweets>0]
as.character(get_sentimentscore(data$text[10])$Degree_of_Emotion[a$Number_of_Tweets>0])
data$text[10]
polarity = vector('list',nrow(data))
for (i in 1:nrow(data)){
polarity[i] = as.character(get_sentimentscore(data$text[i])$Degree_of_Emotion[a$Number_of_Tweets>0])}
data$polarity=polarity[i]
head(data)
runApp()
data = read.csv('user_loc_df.csv',stringsAsFactors = FALSE)
data = data%>%select('name', 'location',
'created_at','text','quoted_followers_count','quoted_retweet_count')
data$text=iconv(data$text, "latin1", "ASCII", sub="")
polarity = vector('list',nrow(data))
for (i in 1:nrow(data)){
polarity[i] = as.character(get_sentimentscore(data$text[i])$Degree_of_Emotion[a$Number_of_Tweets>0])}
data$polarity=polarity[i]
polarity = vector('list',nrow(data))
for (i in 1:nrow(data)){
polarity[i] = as.character(get_sentimentscore(data$text[i])$Degree_of_Emotion[a$Number_of_Tweets>0])}
1:nrow(data)
get_sentimentscore(data$text[21])$Degree_of_Emotion[a$Number_of_Tweets>0])
get_sentimentscore(data$text[21])$Degree_of_Emotion[a$Number_of_Tweets>0]
get_sentimentscore(data$text[21])
get_sentimentscore(data$text[21])$Degree_of_Emotion[Number_of_Tweets>0]
get_sentimentscore(data$text[21])$Degree_of_Emotion[,Number_of_Tweets>0]
a=get_sentimentscore(data$text[21])
as.character(a$Degree_of_Emotion[a$Number_of_Tweets>0])
runApp()
runApp()
polarity = vector('list',nrow(data))
for (i in 1:nrow(data)){
polarity[i] = as.character(get_sentimentscore(data$text[i])$Degree_of_Emotion[get_sentimentscore(data$text[i])$Number_of_Tweets>0])}
data$polarity=polarity[i]
data$polarity
polarity = vector('list',nrow(data))
for (i in 1:nrow(data)){
polarity[i] = as.character(get_sentimentscore(data$text[i])$Degree_of_Emotion[get_sentimentscore(data$text[i])$Number_of_Tweets==1])}
data$polarity=polarity[i]
data = read.csv('user_loc_df.csv',stringsAsFactors = FALSE)
data = data%>%select('name', 'location',
'created_at','text','quoted_followers_count','quoted_retweet_count')
data$text=iconv(data$text, "latin1", "ASCII", sub="")
get_sentimentscore(data$text[10])
get_sentimentscore(data$text[21])
runApp()
th.start <- pi*(1-a/100)
th.start <- pi*(1-30/100)
th.start
th.end   <- pi*(1-70/100)
th.end
th       <- seq(th.start,th.end,length=100)
th
th       <- seq(th.start,th.end,length=1)
th
th       <- seq(th.start,th.end,length=100)
x        <- c(r1*cos(th),rev(r2*cos(th)))
r1=0.5,r2=1.0
r1=0.5
r2=1.0
x        <- c(r1*cos(th),rev(r2*cos(th)))
x
y
y        <- c(r1*sin(th),rev(r2*sin(th)))
y
data.frame(x,y)
ggplot()+
geom_polygon(data=get.poly(breaks[1],breaks[2]),aes(x,y),fill="#008B8B")
get.poly <- function(a,b,r1=0.5,r2=1.0) {
th.start <- pi*(1-30/100)
th.end   <- pi*(1-70/100)
th       <- seq(th.start,th.end,length=100)
x        <- c(r1*cos(th),rev(r2*cos(th)))
y        <- c(r1*sin(th),rev(r2*sin(th)))
return(data.frame(x,y))
}
ggplot()+
geom_polygon(data=get.poly(breaks[1],breaks[2]),aes(x,y),fill="#008B8B")
ggplot()+
geom_polygon(data=get.poly(breaks[1],breaks[2]),aes(x,y),fill="#008B8B")+
geom_polygon(data=get.poly(breaks[2],breaks[3]),aes(x,y),fill="#00CED1")
ggplot()+
geom_polygon(data=get.poly(breaks[1],breaks[2]),aes(x,y),fill="#008B8B")+
geom_polygon(data=get.poly(breaks[2],breaks[3]),aes(x,y),fill="#00CED1")+
geom_polygon(data=get.poly(breaks[3],breaks[4]),aes(x,y),fill="#AFEEEE")+
geom_polygon(data=get.poly(pos-1,pos+1,0.2),aes(x,y))
gg.gauge <- function(pos,breaks=c(0,30,70,100)) {
get.poly <- function(a,b,r1=0.5,r2=1.0) {
th.start <- pi*(1-a/100)
th.end   <- pi*(1-b/100)
th       <- seq(th.start,th.end,length=100)
x        <- c(r1*cos(th),rev(r2*cos(th)))
y        <- c(r1*sin(th),rev(r2*sin(th)))
return(data.frame(x,y))
}
ggplot()+
geom_polygon(data=get.poly(breaks[1],breaks[2]),aes(x,y),fill="#008B8B")+
geom_polygon(data=get.poly(breaks[2],breaks[3]),aes(x,y),fill="#00CED1")+
geom_polygon(data=get.poly(breaks[3],breaks[4]),aes(x,y),fill="#AFEEEE")+
geom_polygon(data=get.poly(pos-1,pos+1,0.2),aes(x,y))+
geom_text(data=as.data.frame(breaks), size=5, fontface="bold", vjust=0,
aes(x=1.1*cos(pi*(1-breaks/100)),y=1.1*sin(pi*(1-breaks/100)),label=paste0(breaks,"%")))+
#annotate("text",x=0,y=0,label=pos,vjust=0,size=8,fontface="bold")+
coord_fixed()+
theme_bw()+
theme(axis.text=element_blank(),
axis.title=element_blank(),
axis.ticks=element_blank(),
panel.grid=element_blank(),
panel.border=element_blank())
}
gg.gauge <- function(pos,breaks=c(0,x,y,z)) {
get.poly <- function(a,b,r1=0.5,r2=1.0) {
th.start <- pi*(1-a/100)
th.end   <- pi*(1-b/100)
th       <- seq(th.start,th.end,length=100)
x        <- c(r1*cos(th),rev(r2*cos(th)))
y        <- c(r1*sin(th),rev(r2*sin(th)))
return(data.frame(x,y))
}
ggplot()+
geom_polygon(data=get.poly(breaks[1],breaks[2]),aes(x,y),fill="#008B8B")+
geom_polygon(data=get.poly(breaks[2],breaks[3]),aes(x,y),fill="#00CED1")+
geom_polygon(data=get.poly(breaks[3],breaks[4]),aes(x,y),fill="#AFEEEE")+
geom_polygon(data=get.poly(pos-1,pos+1,0.2),aes(x,y))+
geom_text(data=as.data.frame(breaks), size=5, fontface="bold", vjust=0,
aes(x=1.1*cos(pi*(1-breaks/100)),y=1.1*sin(pi*(1-breaks/100)),label=paste0(breaks,"%")))+
#annotate("text",x=0,y=0,label=pos,vjust=0,size=8,fontface="bold")+
coord_fixed()+
theme_bw()+
theme(axis.text=element_blank(),
axis.title=element_blank(),
axis.ticks=element_blank(),
panel.grid=element_blank(),
panel.border=element_blank())
}
gg.gauge(value,breaks=c(0,30,70,100))
value=0.8*100
gg.gauge(value,breaks=c(0,30,70,100))
gg.gauge(value,breaks=c(0,0.5,0.7,1))
value=0.08
gg.gauge(value,breaks=c(0,0.5,0.7,1))
value=0.08
gg.gauge(value,breaks=c(0,10,40,100))
gg.gauge(value,breaks=c(0,10,40,40))
gg.gauge(value,breaks=c(0,10,40,10))
gg.gauge(value,breaks=c(0,0.1,0.5,0.5))
gg.gauge(value,breaks=c(0,0.1,0.5,1))
gg.gauge(value,breaks=c(0,10,20,20))
gg.gauge(value,breaks=c(10,10,20,20))
gg.gauge(value,breaks=c(1,10,20,20))
gg.gauge(value,breaks=c(1,10,20,1))
gg.gauge(value,breaks=c(0,0.5,0.7,1))
gg.gauge(value,breaks=c(0,50,70,100))
gg.gauge(value*100,breaks=c(0,50,70,100))
